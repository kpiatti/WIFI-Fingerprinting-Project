---
title: "WIFI Fingerprinting Project"
author: "Katherine Piatti"
date: "4/13/2021"
output: html_document
---

# PROJECT SUMMARY ############################################

The client needs a system to help people navigate complex, unfamiliar interior spaces without getting lost (e.g.industrial campuses, shopping malls, etc.). They would like us to investigate the feasibility of using "wifi fingerprinting". 

>
>
Wifi fingerprinting uses the signals from multiple wifi hotspots within the buildinga to determine location, analogously to how GPS uses satellite signals.

Why not use GPS? While GPS works fairly reliably outdoors, it generally doesn't work indoors.

We have been provided with a large database of wifi fingerprints from a multi-building industrial campus, each fingerprint is associated with a building, floor, and location ID.

Your job is to evaluate multiple machine learning models to see which produces the best result, enabling us to make a recommendation to the client. 



# Dataset ##############################################################

From UCI Machine Learning Repository ([here](http://archive.ics.uci.edu/ml/datasets/UJIIndoorLoc)).

> Automatic user localization consists of estimating the position of the user (latitude, longitude and altitude) by using an electronic device, usually a mobile phone. Outdoor localization problem can be solved very accurately thanks to the inclusion of GPS sensors into the mobile devices. However, indoor localization is still an open problem mainly due to the loss of GPS signal in indoor environments. Although, there are some indoor positioning technologies and methodologies, this database is focused on WLAN fingerprint-based ones (also know as WiFi Fingerprinting).

### Characteristics

- covers 3 buildings with 4 or more floors and almost 110.000m2
- appropriate for classification (e.g. building & floor identification) or regression (e.g. longitude & latitude estimation). 
- created in 2013 by more than 20 users and 25 Android devices. 
- contains 19,937 training/reference records & 1,111 validation/test records and 529 attributes.
- each WiFi fingerprint can be characterized by the *detected Wireless Access Points* (WAPs) and the corresponding *Received Signal Strength Intensity* (RSSI). 
  + intensity values are represented as negative integer values ranging -104dBm (extremely poor signal) to 0dbM. 
  + 100 is used to denote when a WAP was not detected.
  + 520 different WAPs were detected. Thus, each WiFi fingerprint is composed by 520 intensity values.
- the coordinates (latitude, longitude, floor) and Building ID are the attributes to be predicted.
- type of space (e.g. office, lab) and relative position to the space (inside/outside) are included. Note: "outside" means that the capture was taken in front of the door to the space.
- Information about the users, as well as how (android device & version) and when (timestamp) WiFi capture was taken is also included.





## Load Pkgs & Data

```{r}
library(tidyverse)
library(here)
library(skimr)
library(broom)
library(janitor)
library(caret)

library(devtools)
 # installed pkg ggbiplot w/ install_github("vqv/ggbiplot") but when i loaded it there were conflicts with dplyr, so i unloaded it w/ detach("package:ggbiplot", unload=TRUE)
```


```{r}
#read in dataset
wifi <- read.csv(here("data", "trainingData.csv"))

#convert to tibble
as_tibble(wifi)
```






# DATA WRANGLING ##########################################

```{r}
# standardize var names  
wifi <- wifi %>%
  clean_names()

# remove unnecessary vars
wifi01 <- wifi %>% 
  select(-userid, -phoneid, -timestamp, -longitude, -latitude)

# return names of last 10 vars  
wifi01 %>%
  names() %>% 
  tail(10)
```




## Reduce WAP Variables #####################

There are 520 wifi access point (WAP) variables in the dataset. Using that many variables during modeling will be very computationally taxing and thus time consuming. 




### Zero & Near Zero Variance ######################

```{r}
# find and remove columns with zero variance
wifi02 <- wifi01 %>% 
  select(-(which(apply(wifi01, 2, var) == 0)))
```
That removed 55 WAP variables that had zero variance. Variables without variance are useless for modeling and cause problems for PCA analysis.

```{r}
# look for features with near zero variance
wifi02 %>% 
  select(wap001:wap519) %>% 
  nearZeroVar(freqCut = 95/5,
              uniqueCut = 95,
              saveMetrics = TRUE,
              foreach = TRUE,
              allowParallel = TRUE) %>% 
  arrange(desc(percentUnique))
```

There are 351 WAP variables with variance equal to or greater than 5%.

```{r}

```

```{r}
wifi02 %>% 
  select_if(nearZeroVar(percentUnique), any(percentUnique >= 0.05))
```




### Principle Component Analysis (PCA) ##################

Let's try to reduce the number of variables by selecting only the most important ones using principle component analysis (PCA). Which identifies the variables that have the most variance/ 

```{r}
# produce the PCA
pca_fit <- wifi02 %>% 
  prcomp(center = TRUE, scale = TRUE)


# plot the data in PC coordinates
pca_fit %>%
  augment(wifi02) %>% # add original dataset back in
  ggplot(aes(.fittedPC1, .fittedPC2, color = buildingid)) + 
  geom_point(size = 1.5)
 ```


#### Extract PCA Components ##################

*Rotation Matrix*
```{r}
# extract rotation matrix
pca_fit %>%
  tidy(matrix = "rotation")
```


*Explained Variance*
```{r}
# create tibble of variance explained by each component
pca_fit %>%
  tidy(matrix = "eigenvalues")
```
PCA reveals that 321 components capture about 95% of the variance, and 258 components capture about 90% of the variance. 



### Summary ###############################



## Separate Data by Building  #################

The full dataset is too large to use with classification models on my home computer. So I'll only work with data for 1 of 3 buildings. 
```{r}
#plot building counts
qplot(wifi02$buildingid)
```

Buildings 0 and 1 have fewest number of observations, so modeling for one of them will be more tractable.

```{r}
#filter data for building 0
b0_wifi02 <- wifi02 %>% 
  filter(buildingid == 0)

#filter data for building 1
b1_wifi02 <- wifi02 %>% 
  filter(buildingid == 1)

#filtrt data for building 2
b2_wifi02 <- wifi02 %>% 
  filter(buildingid == 2)
```

For the purposes of this project I'm going to focus on building 1. 



### Zero Variance ############################

```{r}
# remove vars with zero variance from building 1 dataframe
b1_wifi03 <- b1_wifi02 %>% 
  select(-(which(apply(b1_wifi02, 2, var) == 0)))
```
Removed 259 variables with zero variance.


```{r}
# remove vars with zero variance from building 0 dataframe
b0_wifi03 <- b0_wifi02 %>% 
  select(-(which(apply(b0_wifi02, 2, var) == 0)))
```
Removed 266 variables with zero variance. 


```{r}
# remove vars with zero variance from building 2 dataframe
b2_wifi03 <- b2_wifi02 %>% 
  select(-(which(apply(b2_wifi02, 2, var) == 0)))
```
Removed 263 variables with zero variance. 




## Create Unique Location ID

Because we are trying to predict the accuracy of wifi fingerprinting technology, we need to create a variable that uniquely identifies each location in the dataframe. There are 6 variables related to location in the dataframe:
     
     - floor
     - buildingid
     - spaceid
     - relativeposition
     - longitude
     - lattitude

I cannot use the longitude and lattitude variables because their values are continuous and I am using a classification model to assess the accuracy of the wifi fingerprinting system. 

So I need to construct a location variable out of the floor, building, spaceid, and relative position variables. The first step is to use the dplyr unite funtion to combine them into one new variable called locationid.

```{r}
#unite 4 location vars into single locationid
wifi03 <- wifi02 %>% 
  unite(locationid, c(floor, buildingid, spaceid, relativeposition))
```

```{r}
# check class of new var
class(wifi03$locationid)
```


The next step is to coerce locationid from a character type to a factor type variable.
```{r}
wifi04 <- wifi03$locationid %>% 
  as_factor()
```




# MODELING ##########################################


# MODEL EVALUATION ########################## 
